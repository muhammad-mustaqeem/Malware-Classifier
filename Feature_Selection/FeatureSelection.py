from typing import List, Union

import pandas as pd
from rgf import RGFClassifier
from scipy.stats import entropy
from sklearn.utils import check_random_state

from xgboost import XGBClassifier


def apply_frequency_threshold(dataset: pd.DataFrame) -> pd.DataFrame:
    """
    Apply frequency threshold to the dataset by dropping columns based on the summed values.

    Args:
        dataset: A pandas DataFrame representing the dataset.

    Returns:
        A new pandas DataFrame with columns dropped based on the frequency threshold.
    """
    columns_to_drop: List[str] = []
    for feature in dataset:
        summed: Union[int, float] = sum(dataset[feature])

        if summed < 100:
            columns_to_drop.append(feature)
    return dataset.drop(columns_to_drop, axis=1)


def filter_matrix(dataset: pd.DataFrame) -> pd.DataFrame:
    """
    Filter the dataset by dropping columns and rows based on certain criteria.

    Args:
        dataset: A pandas DataFrame representing the dataset.

    Returns:
        A new pandas DataFrame with columns and rows filtered based on the criteria.
    """
    files = list(dataset['File Name'])

    # Drop unnecessary columns
    dataset.drop(['File Name', 'Target'], axis=1, inplace=True)

    sums = list(dataset.sum(axis=1))

    # Create a new DataFrame to store file names and corresponding sums
    ans = pd.DataFrame({'File': files, 'Sum': sums})

    # Identify files with zero sums
    zero_files = list(ans[ans['Sum'] == 0].index)
    print(f"{len(zero_files)} Files with 0 features\n")

    # Drop rows with zero sums
    dataset.drop(zero_files, inplace=True)

    return dataset


def dataset_entropy(dataset: pd.DataFrame, k_best: int) -> List[str]:
    """
    Calculate the entropy for each feature in the dataset and return the top K features with highest entropy.

    Args:
        dataset: A pandas DataFrame representing the dataset.
        k_best: An integer indicating the number of top features to return.

    Returns:
        A list of the top K features with highest entropy.
    """
    entropy_values = []

    # Calculate entropy for each column/feature in the dataset
    for column in dataset:
        p_data = dataset[column].value_counts()
        entropy_values.append(entropy(p_data))

    # Create a DataFrame to store feature names and corresponding entropy values
    results = pd.DataFrame(data={'Feature': dataset.columns, 'Entropy': entropy_values})

    # Sort the DataFrame by entropy in descending order
    results = results.sort_values(by='Entropy', ascending=False)

    # Return the top K features with highest entropy
    return list(results.head(k_best)['Feature'])


import time
import sys
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from typing import List


def wrapper_method_rfc(X_val: pd.DataFrame, y_val: pd.Series, minimum_features: int) -> List[str]:
    """
    Apply the Wrapper Method using Random Forest Classifier to select the best features.

    Args:
        X_val: A pandas DataFrame representing the feature matrix.
        y_val: A pandas Series representing the target variable.
        minimum_features: An integer indicating the minimum number of features to select.

    Returns:
        A list of the best features selected by the Wrapper Method.
    """
    max_accuracy = 0
    total_time = 0
    count = 1
    best_features = []

    # Continue the loop until the number of columns exceeds the minimum features threshold
    while len(X_val.columns) > minimum_features:
        end = len(X_val.columns)
        start = time.time()

        # Split the dataset into train and test sets
        X_train, X_test, y_train, y_test = train_test_split(X_val, y_val, shuffle=True,
                                                            stratify=y_val, test_size=0.2)

        # Train a Random Forest Classifier
        rfc = RandomForestClassifier(random_state=0, n_jobs=-1)
        rfc.fit(X_train, y_train)

        # Calculate accuracy score on the test set
        acc_score = rfc.score(X_test, y_test)

        # Update the best accuracy and corresponding best features
        if acc_score > max_accuracy:
            max_accuracy = acc_score
            best_features = X_train.columns

        # Calculate feature importance's
        importances = rfc.feature_importances_

        # Create a DataFrame to store feature importances
        feature_importances = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})

        # Find the feature with the least importance
        least_important = feature_importances['Feature'][feature_importances[
            feature_importances.Importance == min(feature_importances['Importance'])].index]

        # Check if removing the least important feature violates the minimum features threshold
        if len(X_val.columns) - len(least_important) < minimum_features:
            break

        # Drop the least important feature from the feature matrix
        X_val = X_val.drop(least_important, axis=1)
        ending = time.time()
        total_time += ending - start
        average_time = ((total_time / count) * end) / 60

        sys.stdout.flush()
        print(
            f"\rIteration = {count}\t\tFeatures Left = {end}\t\tETA = {round((average_time - (total_time / 60)), 2)} min"
            f"\t\tElapsed = {round((total_time / 60), 2)} min\tBest Accuracy '{round(max_accuracy * 100, 5)}' "
            f"with '{len(best_features)}' Features ",
            end="")
        start = ending = 0
        count += 1

    return list(best_features)


def wrapper_method_rgf(x_val: pd.DataFrame, y_val: pd.Series, minimum_features: int) -> List[str]:
    """
    Apply the Wrapper Method using Regularized Greedy Forest (RGF) Classifier to select the best features.

    Args:
        x_val: A pandas DataFrame representing the feature matrix.
        y_val: A pandas Series representing the target variable.
        minimum_features: An integer indicating the minimum number of features to select.

    Returns:
        A list of the best features selected by the Wrapper Method.
    """
    max_accuracy = 0
    total_time = 0
    count = 1
    best_features = []

    # Continue the loop until the number of columns exceeds the minimum features threshold
    while len(x_val.columns) > minimum_features:
        end = len(x_val.columns)
        start = time.time()

        # Split the dataset into train and test sets
        x_train, x_test, y_train, y_test = train_test_split(
            x_val, y_val, shuffle=True, stratify=y_val, test_size=0.2, random_state=check_random_state(None)
        )

        # Train an RGF Classifier
        rgf = RGFClassifier(loss='LS', algorithm='RGF_Sib', max_leaf=1000, n_jobs=-1)
        rgf.fit(x_train, y_train)

        # Calculate accuracy score on the test set
        acc_score = rgf.score(x_test, y_test)

        # Update the best accuracy and corresponding best features
        if acc_score > max_accuracy:
            max_accuracy = acc_score
            best_features = x_train.columns

        # Calculate feature importances
        importances = rgf.feature_importances_

        # Create a DataFrame to store feature importances
        feature_importances = pd.DataFrame({'Feature': x_train.columns, 'Importance': importances})

        # Find the feature with the least importance
        least_important = feature_importances['Feature'][feature_importances[
            feature_importances.Importance == min(feature_importances['Importance'])].index]

        # Check if removing the least important feature violates the minimum features threshold
        if len(x_val.columns) - len(least_important) < minimum_features:
            break

        # Drop the least important feature from the feature matrix
        x_val = x_val.drop(least_important, axis=1)
        ending = time.time()
        total_time += ending - start
        average_time = ((total_time / count) * end) / 60

        sys.stdout.flush()
        print(
            f"\rIteration = {count}\t\tFeatures Left = {end}\t\tETA = {round((average_time - (total_time / 60)), 2)} min"
            f"\t\tElapsed = {round((total_time / 60), 2)} min\tBest Accuracy '{round(max_accuracy * 100, 5)}' "
            f"with '{len(best_features)}' Features ",
            end="")

        count += 1

    return list(best_features)


def wrapper_method_xgb(x_val: pd.DataFrame, y_val: pd.Series) -> List[str]:
    """
    Apply the Wrapper Method using XGBoost Classifier to select the best features.

    Args:
        x_val: A pandas DataFrame representing the feature matrix.
        y_val: A pandas Series representing the target variable.

    Returns:
        A list of the best features selected by the Wrapper Method.
    """
    x_train, x_test, y_train, y_test = train_test_split(
        x_val, y_val, shuffle=True,
        stratify=y_val, test_size=0.2
    )
    params = {
        'objective': 'multi:softprob',
        'eval_metric': 'error',
        'silent': 1,
    }

    model = XGBClassifier(**params, verbosity=2, n_jobs=-1)
    model.fit(x_train, y_train)

    print('XGB Final Accuracy =', model.score(x_test, y_test))

    importance = model.feature_importances_
    importance_data_frame = pd.DataFrame(columns=['Feature', 'Importance'])
    importance_data_frame['Feature'] = x_train.columns
    importance_data_frame['Importance'] = importance
    mean_imp = importance_data_frame['Importance'].mean()

    non_important_features = list(importance_data_frame[importance_data_frame['Importance'] == 0].index)

    print("Zero Importance =", len(non_important_features))
    print('Mean =', mean_imp)

    best_features = importance_data_frame['Feature'][
        importance_data_frame[importance_data_frame['Importance'] > mean_imp].index
    ]

    print(
        f'Greater than Mean = {len(list(importance_data_frame[importance_data_frame["Importance"] > mean_imp].index))}')
    print(best_features)

    return list(best_features)


def feature_selection(file_path: str, rfc_threshold: int, entropy_threshold: int, rgf_threshold: int) -> list:
    """
    Perform feature selection using various methods and return the best features.

    Args:
        file_path: The file path to the dataset.
        rfc_threshold: The threshold value for the Random Forest Classifier method.
        entropy_threshold: The threshold value for the Entropy method.
        rgf_threshold: The threshold value for the Regularized Greedy Forest method.

    Returns:
        A list of the best features selected by combining the results of different feature selection methods.
    """
    op_code_dataset = pd.read_csv(file_path, delimiter=',')
    copy_dataset = op_code_dataset.copy()
    op_code_dataset = filter_matrix(op_code_dataset)

    x = op_code_dataset.drop(['Target', 'File Name'], axis=1)
    y = op_code_dataset['Target']

    rfc_results = wrapper_method_rfc(x, y, rfc_threshold)
    entropy_results = dataset_entropy(x, entropy_threshold)
    xgb_results = wrapper_method_xgb(x, y)
    rgf_results = wrapper_method_rgf(x, y, rgf_threshold)

    best_features = list(set().union(rfc_results, xgb_results, entropy_results, rgf_results))

    return best_features


Results = pd.DataFrame(data=feature_selection('../Feature Matrix/DllMatrix', 4000, 1500, 1000))
Results.to_csv('../Feature Selection/Best_API_Features.csv', header=None, index=None)
